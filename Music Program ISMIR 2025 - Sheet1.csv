Title,Live / Video,Performer,Abstract,Bio,Video URL,Thumbnail photo,Tech rider
Music Program,,,,,,,
Mind Improvisation II: Real-Time Conceptual Transformation through fNIRS and MAX/MSP,Live,"Hyo Jee Kang, Jinok Jo, Joohun Lee ","Mind Improvisation II,' building on the concepts of 'Mind Improvisation' and 'Growing Seeds,' merges neural signal processing with embodied performance.Using a 15-channel functional near-infrared spectroscopy (fNIRS) device (NIRSIT Lite), the performer’s prefrontal cortex activity is streamed live to a custom Max/MSP environment. The performer’s ΔHbO signals are interpreted not as binary triggers but as expressive, continuous parameters—modulating filters, synthesis engines, and real-time audio morphing tools. Simultaneously, a Yamaha Disklavier fulfills a dual role: As a readymade improviser, it acoustically replays pre-recorded performances—the “ghost” layer. As a live acoustic instrument, it is physically played by the performer in real time. These layers—brain-driven synthesis and live/automated piano—are recursively shaped by the performer’s evolving neural states. Auditory output and visual feedback, in turn, influence the brain, forming a perceptual-cognitive feedback loop. The system embodies the concept of transformation as both process and event, echoing the principles of Process Art and Deleuzian deterritorialization. Brain signals are treated not as mere control data but as an expressive medium, enabling a decentered and emergent artistic language. This work challenges conventional authorship by positioning the brain as both subject and source of the performance. The audience witnesses an improvisation sculpted by moment-to-moment mental fluctuations, transforming not a metaphor, but an immediate, embodied experience","Hyo Jee Kang (강효지)
Role: Composer & Performer — composotion, performance, biomedical-media research
Bio: Hyo Jee Kang is an associate professor at Korea National University of Transportation and an active pianist and performer. Trained in Korea and Germany (DMA, Konzertexamen), she combines piano performance with media art, biomedical-signal projects (e.g., fNIRS-based studies), VR/AR, and interdisciplinary collaborations that fuse music, technology, and performance.

Jinok Cho(조진옥)
Role: Sound Artist & Audio Realizer — specializes in audio realization, Max/MSP programming, and live signal processing.
Bio: He is a Composer and Sound designer (formerly a researcher at Seoul National University Arts & Science Center) who produces electroacoustic and multimedia works. His projects focus on immersive audio design using Max/MSP and live processing, fixed media, and experimental soundscapes that often reinterpret Korean traditional narratives such as Sugunga.

Joohun Lee (이주헌)
Role: Media Artist — visualization, interactive media, and AR/VR/new-media content development.
Bio: Juhun Lee is a professor at Dong-Ah Institute of Media & Arts specializing in visualization and interactive media. He develops immersive visualization systems, AR/VR environments, and collaborative metaverse/virtual-space projects while teaching new-media content production and real-time media techniques.",https://www.youtube.com/watch?v=BU6B-di-PJY,https://drive.google.com/file/d/1ZjBost8ors0Ew67kr3p-r1q_WsQxbdtA/view?usp=drive_link,"- Projector & PA
- 1 x Microphone
- 1 x Standard table
- 1 x Large table
- 3 x Chairs
- Disklavier

Full tech rider doc: https://drive.google.com/file/d/1YAnwATxCebgb4_RrM0FRQUlcPlvcXGA3/view?usp=drive_link"
Aleatorica: A Mosaic of Popular Interval Content,Video,Valerie Sazonova,"Aleatorica is a mosaic-like music map that visualizes and sonifies the interval content of a large dataset of music by aleatorically regenerating its melody, creating a sonic space for exploring the continuum of melodic features in contemporary music. Following the idea of a 'lecture-concert', this performance will touch on the computational and music theoretical methods used to create such an interface. In order to create the map, a large dataset of MIDI format songs was collected and processed into interval distributions based on note transition frequency. Afterwards, Earth Mover’s Distance is computed between distributions using a music-theory informed cost matrix and subsequently used in the UMAP algorithm to generate a latent representation of the dataset. Afterwards, a tiled design is generated using Voronoi tessellation on the dimensionality reduced plot and colored based on quantifiable music theoretical features on the interval distributions. Finally, the interval distribution is used to derive a Markovian transition matrix, which can be used to aleatorically generate music. Additionally, Aleatorica will be available as a website, promoting audience interaction and personal discovery. The resulting performance is a compelling inspection of the landscape of modern melody, blurring the lines between music data visualization and data-driven sound art.","Valerie Sazonova is an undergraduate student at the University of Alabama pursuing a degree in Computer Engineering and Mathematics. She's interested in exploring the intersection of technical research and creative practice, focusing on bridging computational approaches from music information retrieval and traditional music-theoretical analysis. Her research experience ranges from applying machine learning for radar-based sign language recognition to contributing as a student researcher at NYU's Music and Audio Research Laboratory. Additionally, she has collaborated with artists, designing novel electronic musical interfaces, composing for unconventional instruments, and building self-oscillating sound art installations.",https://drive.google.com/file/d/1RGsTfYrPNBXqAHtgFgXGx9oWzHqSPyho/view?usp=sharing,https://drive.google.com/file/d/1j6oCPYszt0PWiomZp9eO5VTl2sIpIhYA/view?usp=drive_link,- Video(Projector & PA)
Melody Slot Machine with RoboSax,Live,"Masatoshi Hamanaka, Gou Koutaki","Melody Slot Machine with RoboSax is our AI-based Melody Slot Machine that controls a robot-based RoboSax. Melody Slot Machine is a dial with the staves of music displayed on an iPad, which can be rotated to change the melody variations. The melody variations are generated on the basis of the AI-based melody-morphing method and can be partially switched to another variation without any significant change in the overall melody structure and with no musical breakdown. The microcomputer on the RoboSax receives the MIDI note from Melody Slot Machine and moves the servomotor so that the fingering corresponds to the note number.","Masatoshi Hamanaka received a Ph.D. in Engineering from the University of Tsukuba, Japan, in 2003. He is currently a leader of the Music Information Intelligence team at Center for Advanced Intelligence Project, RIKEN. His research interest is in music information technology and biomedical and unmanned aircraft systems. He received the Journal of New Music Research Distinguished Paper Award in 2005, SIGGRAPH2019 Emerging Technologies Laval Virtual Revolution Research Jury Prize in 2019, IJCAI-19 Most Entertaining Video Award in 2019, Augmented Human International Conference Best Poster Paper Award in 2021, and International Conference on Multimedia Modeling Best Demonstration Award in 2024. 

Gou Koutaki received a Doctor of Engineering from Kumamoto University, Japan, in 2007. He joined the Production Engineering Research Laboratory, Hitachi, Ltd., in 2007 and is currently a professor at Kumamoto University. His research interests include image processing and musical-instrument support systems. He was originally a researcher in computer vision and has presented papers at CVPR, ICCV, SIGGRAPH technical paper, IJCV, etc., but is currently designing and manufacturing robotic instruments.

The four performers of the Robo Sax Quartet are as follows. They are undergraduate students at Kumamoto University and members of the Wind Orchestra Club.

Kotomi Hisano, Robo-Soprano Sax
Ambi Tanaka, Robo-Alto Sax
Sho Fujii, Robo-Tenor Sax
Keisuke Mizuta, Robo-Baritone Sax", https://www.youtube.com/watch?v=n_Qd66uhY70 ,https://drive.google.com/file/d/181aWDhTXbo8k6pIQo8Qm5Qh-p3A05z3x/view?usp=drive_link,"- 4 x Variable chairs for saxonphonists
- Use GoPro with iPad to project to a projector screen

Full tech rider doc: https://drive.google.com/file/d/1aN7gReYvWgKB6qAdRvNSPvP3V-u-jmGE/view?usp=drive_link"
"One, Two, Many",Video,Oded Ben-Tal,"One, Two, Many is a three-way partnership between two human performers and an Artificial ‘Intelligence’. One of the main challenges in human - AI co-creation is the question of motivation, goals, or aesthetic preferences. Cutting-edge AI systems are very good imitation systems increasingly able to reproduce human-like outputs. But computers are obviously incapable of aesthetic judgements or goals. In One, Two, Many the AI systems joins the flute players in realising the piece as a partner defined by an aesthetic preference – that of an impatient listener, one who seeks novelty. First the AI ‘listens’ to each flute and evaluates similarity and surprise in real-time. Stable input – signals that are predictability or similar to previous signals – increases the ‘boredom’ in the system. When this boredom gets over a defined threshold the computer changes it internal setting to achieve musical change and resets the boredom. The computer, therefore, interprets an audio signal and changes it’s own musical behaviour based on this interpretation: a system that listens and responds according to some aesthetic criteria. While the realisation of this piece involves a partnership between the human performers and the AI, this is an unequal partnership. First, the AI is only able to transform the flute sounds not initiated sounds on its own. Second, the aesthetic preference in the system are obviously rather simple and lack the nuance, sophistication, or experience of a human musician. Since listening is at the heart of the multi-party interaction envisioned, the score is designed to allow the performers a degree of freedom. Freedom to respond to and try to influence the AI as well as ability to respond to each other. The parts are loosely coordinated – pages act as coordination units. But within each page players have some optional figures, alternatives and choice of repetition. The performers, therefore, are able to control, to a degree, the similarity/change in the music and thus interact with the aesthetic ‘preferences’ of the AI.","Oded Ben-Tal is London-based composer and researcher working at the intersection of music, computing, and cognition. His compositions range from purely acoustic pieces, to interactive, live electronic pieces and multimedia work. In recent years he is particularly interested in the interaction between human and computational creativities: Applying deep learning techniques to folk musics and interrogating the creative capacity of the resulting generative system within the folk tradition as well as outside it; Using AI-inspired approaches in the domain of interactive, live electronic music and joint improvisation between human performers and a semi-autonomous AI system. His work has been supported by grants from the UK’s Arts and Humanities Research Council, the Leverhulme Trust and the Volkswagen Foundation. He is an Associate Professor in the Department of Performing Arts, Kingston University, London.",https://drive.google.com/file/d/1dwAs2q0noWrJ3N3cMwKM45sVYqA9hJEN/view,https://drive.google.com/file/d/1ouYNeWBTQZZ339VYGA7Dw1f1EDafOe9r/view?usp=drive_link,- Video (Projector & PA)
Jam Session Opening,,,,,,,
The JAM_BOT: An Insight Into An AI-Human Co-Created Musical Improvisation,Live,"JamBot (Lancelot Blanchard,
Perry Naseck)","This show is a demonstration of the JAM_BOT, a real-time system for collaborative free improvisation with music language models that was developed as part of a collaboration between MIT Media Lab researchers and GRAMMY-winning visiting artist Jordan Rudess. The JAM_BOT accompanies the improvisation of live performers onstage using optimized Transformer-based Music Language Models. It was first demonstrated publicly during a sold-out performance held at the MIT Media Lab on September 21, 2024. This ISMIR 2025 show intends to reproduce segments of the original show to demonstrate the potential of the system to the ISMIR scientific community.","Perry Naseck, pnaseck@media.mit.edu, Research Assistant, MIT Media Lab.
Perry Naseck is an artist, engineer, and researcher working with interactive, kinetic, light- and time-based media. He specializes in interaction, orchestration, and animation of systems of sensors and actuators for live performance and permanent installation. Perry is pursuing a PhD in the Responsive Environments group at the MIT Media Lab where he focuses on creating new experiences for musicians and audiences. His research explores how to closely link live performance visual systems with a focus on improvised and AI-generated music. Perry completed a Bachelor's of Engineering Studies and Arts at Carnegie Mellon where he studied both Art and Electrical & Computer Engineering.

Lancelot Blanchard, lancelot@media.mit.edu, Research Assistant, MIT Media Lab.
Lancelot Blanchard is a musician, engineer, and AI researcher pursuing a PhD at MIT Media Lab’s Responsive Environments group. His research explores the development of generative AI systems that can enhance the creative processes of musicians, with a special focus on live performances. Lancelot has collaborated with multiple GRAMMY-winning artists to create innovative tools for real-time AI-augmented music making. Lancelot holds a Master's of Research in Artificial Intelligence and Machine Learning and a Master's of Engineering in Computing from Imperial College London, as well as a degree in classical piano from the Conservatory of Rennes, France.",https://vimeo.com/1107163458/c358b5432b,https://drive.google.com/file/d/12TjvqXlD5HOQAVyboiT4qNb7ntRFcXR_/view?usp=drive_link,* See detailed tech rider document: https://drive.google.com/file/d/1rYC1rlB7QYqWHKz1jVuDeqBthXzVBXUl/view?usp=drive_link